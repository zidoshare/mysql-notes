### 什么是事务

说到事务，首先想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）

MySQL 事务提供三个基本的事务指令：

* 可以使用 `start transaction` 或者 `begin` 开启事务。
* 使用 `commit` 提交当前事务，此时的数据才会真正进入持久化的流程
* 使用 `rollback` 回滚当前事务，事务的修改会被取消
* `SET autocommit` 可以启用或者禁用自动提交事务模式

默认，autocommit 是被开启的，这就保证了单个语句具有原子性，当然了，这也意味着每个语句执行成功就不能使用 `rollback` 进行回滚了。但是语句如果执行出错，还是会被自动回滚的。如果这个时候使用开启事务的命令，autocommit 就会被禁用，一直到提交或者回滚事务。

#### 原子性

> 原子性概念很简单,即事务中的所有操作要么一起都完成，要么一起都不完成，不可分割。

做一个尝试，见 [1.原子性](./1.%E5%8E%9F%E5%AD%90%E6%80%A7)

#### 一致性

> 一致性是指在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。

#### 隔离性

> 一致性是指数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。

验证代码 [2.隔离性](./2.%E9%9A%94%E7%A6%BB%E6%80%A7)

#### 持久性

> 持久性是指事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

### 四个隔离级别

在理解隔离界别前，我们首先要了解数据库事务所面临的问题：脏读、幻读、不可重复读

**什么是脏读、幻读、不可重复读呢？**

* 脏读：脏读是指一个事务中访问到了另外一个事务未提交的数据。理解:两个事务完全没有隔离，彼此操作都完全看得见，这就导致了并发情况下数据不一致
* 幻读：一个事务读取 2 次，得到的记录条数不一致。理解：两个事务隔离了对于新增、删除数据的访问。也就是说就算其他事务新增、删除了数据，我查询一个范围的时候查到的还是我事务开始时的那些数据，不会因为其他事务的增删而导致结果出错
* 不可重复读：一个事务读取同一条记录 2 次，得到的结果不一致。理解：两个事务隔离了同一条记录修改，也就是说就算其他事务修改了，我看到的还是之前的，这样就能保证我针对这一条数据的逻辑一定准确

**脏读、幻读、不可重复读产生了什么问题呢？**

要回答这个问题，首先应该了解并发的概念：如果我们的所有业务请求都是**原子性**的，那么我们完全不需要担心数据的操作会超出我们的预期，例如一个业务设置某个数为 2，不论十个请求还是一亿个请求进来都无所谓，反正先来后到，就算你一起来都行，最终都是一步到位。但是事与愿违，几乎不可能有这么简单的业务，起码也是 `先查一查数据是不是为2，如果是2就设置为4，否则设置为8` 这种级别，这时候就不能不先 getData(),然后 setData()了，正常情况下是这样，但是如果两个请求一起进来呢？理想情况下我们觉得因该是是 a 请求先查询是 2 然后修改成 4，然后 b 请求查询到 4 修改成 8，或者反过来 b 请求先查询是 2 然后修改成 4，然后 a 请求查询到 4，修改成 8。然而，事与愿违的是有可能 a 请求查了查是 2，此时 b 请求也查了查，嗯，是 2，然后 b 请求率先把 2 改成了 4。此时 a 请求才慢悠悠的过来改数据（**因为并发情况下，没有人能预测谁会先走到哪一步**），因为之前 a 已经看到了是 2，所以就信誓旦旦的直接设置成 4。这时，问题就来了，请问，**这段逻辑是正确的吗**？

你可能会说，其实没什么问题，影响不大，他们单个的逻辑其实是正确的，也无所谓，修改个数字而已，大不了再来一次。确实，很多情况下，就是这样，没毛病，就像电脑系统出了问题大不了重启，这种情况下，虽然逻辑可能是错误的，但是最终结果其实无关紧要，这也就是**可容忍的**

但是如果把这个数字换成自己的账户，那就**不可容忍**了

从上面的问题，可以看出，我们看问题时一定要看到这个逻辑是不是可容忍的错误，然后根据开发难度（你可能是个完全不会并发的小白）、执行效率（ab 之间都互相不管对方做了啥都坚持自我其实效率是最高的）、安全性（如果是我的钱包缩水那绝对不能容忍了）等方面综合评估。

MySQL 无法预估开发者的容忍度，而要一步到位完全解决这三个问题，那带来的效率损失不可估量，所以它为你提供了一步一步降低错误出错的方案，帮助你去做到你最想要的权衡，这也就是事务隔离性：

MySQL 提供了四个隔离级别：

* `Read Uncommitted`:读未提交
* `Read Committed`: 读已提交
* `Repeatable Read`: 可重复读
* `Serializable`: 串行化

根据验证 [2.隔离性](./2.%E9%9A%94%E7%A6%BB%E6%80%A7)所得出的结果来看，四个隔离级别对应的结果：

* [读未提交](./2.%E9%9A%94%E7%A6%BB%E6%80%A7/1.%E8%AF%BB%E6%9C%AA%E6%8F%90%E4%BA%A4)：事务 1 可以读取到事务 2 修改过但未提交的数据（产生脏读，幻读，不可重复度）
* [读已提交](./2.%E9%9A%94%E7%A6%BB%E6%80%A7/2.%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4)：事务 1 只能在事务 2 修改过并且已提交后才能读取到事务 2 修改的数据（产生幻读，不可重复度）
* [可重复读](./2.%E9%9A%94%E7%A6%BB%E6%80%A7/3.%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB)：事务 1 只能在事务 2 修改过数据并提交后，自己也提交事务后，才能读取到事务 2 修改的数据(产生幻读)
* [串行化](./2.%E9%9A%94%E7%A6%BB%E6%80%A7/4.%E4%B8%B2%E8%A1%8C%E5%8C%96)：事务 1 在执行过程中完全看不到事务 2 对数据库所做的更新。当两个事务同时操作数据库中相同数据时，如果事务 1 已经在访问该数据，事务 2 只能停下来等待，必须等到事务 1 结束后才能恢复运行

但是在这里我建议记忆的顺序应该是**脏读、不可重复读、幻读**，每个隔离级别依次递增的解决每一个问题，最后通过串行化，完美解决三个问题。

### 事务的实现原理

> 实现原理其实不用到处找，官方文档就有，这里我是稍微整理了一下，具体查看地址 [https://dev.mysql.com/doc/refman/5.7/en/mysql-acid.html](https://dev.mysql.com/doc/refman/5.7/en/mysql-acid.html)

#### 原子性原理

原子性其实代表的就是 commit 和 rollback 两个操作。对于此，MySQL 提供一个 undo log(回滚日志)的日志来实现原子性，undo log 是 MySQL innodb 存储引擎所携带的日志

当事务对数据库进行修改时，InnoDB 会生成对应的 undo log，然后如果需要回滚，就会调用利用 undolog 进行反向操作从而达到回滚的目的。

例如当 update 时，会记录被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到 update 之前的状态。

知道可以回滚了，接下来就自然想到一种情况，如果事务 a 修改了列，但未提交，事务 b 再次修改此列，会发生什么情况呢？该怎么回滚呢？

根据这种思路，我做了一个尝试，重现代码见 [3.修改同列](./3.%E4%BF%AE%E6%94%B9%E5%90%8C%E5%88%97)

可以看到，当在两个事务中尝试修改同一行数据时，第二次修改等待锁，而尝试了各种隔离级别后，均会如此，也就是说，事务 b 将在事务 a 执行过程中，为了保证一致性，不会允许事务 b 修改事务 a 中修改的行，即使是最低的隔离级别。

由此可见，实现原子性的第二个关键就是锁。它保证事务 B 中的操作不会干扰事务 a，保证操作整体的原子性。

#### 持久性原理

持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。要实现这一点，就必须保证每一次写入都被持久化下来。

众所周知，数据库的主要瓶颈就是 io，MySQL 提高性能关键就在于减少 io。如果是为了持久性就让每次写入都放进磁盘，那么数据库效率将极低，为了能够提高效率，首先就会想到，把数据的写入放到内存中，然后定期一起写进磁盘。而 MySQL 由此提出的解决方案为 Buffer Pool,Buffer Pool 中包含了磁盘中部分数据页的映射, 当取数据时，先从 buffer pool 中读取，如果没有则去数据库读取然后放入到 buffer pool 中，请注意，这里一般取是直接取一页，也就是说可能不止你查询的数据，而是可能更多的数据取到了 buffer 中。当写入数据时，会**先写入到 buffer pool** 中。

其中，**先写入到 buffer pool** 这一部分我标记了一下，这里再展开说说详细过程，buffer pool 包含一部分特殊的 buffer，它叫 `Change Buffer`。change buffer 记录了那些对使用二级索引但数据没有在 Buffer Pool 的数据进行增、删、改的**操作**。这句话很绕，但是有几个关键点，必须分开来说，首先是记录增删改操作，这个很好理解，如果我们的操作每一次都直接落实到了磁盘，那么必然会导致 io 飙升，这显然是不行的，为此，才把增删改操作给放进 change buffer。其次是没有在 buffer pool 中的数据，这也很容易理解，因为如果数据就已经在内存中了，那么本身就不必写入磁盘，写完内存之后，再由 buffer pool 统一刷数据进磁盘就好。那就难在这句二级索引，为什么要是二级索引，不能是主键索引呢？首先根据前面说的，数据此时仅在磁盘中，并没有在 buffer pool 中，试想，如果要插入一个表中已经存在的主键的行，那么首先就会需要判断主键是否存在，此时就一定需要读取磁盘了，既然都已经读取了磁盘，也就可以直接放在 buffer pool 中了，那么直接使用 buffer pool 还更快，也就不需要这个 change buffer 了，然而对于二级索引通常是没有唯一需求的，并且插入二级索引通常是随机 io，而不像主键索引有顺序，我们都知道顺序 io 速度要远大于随机 io 的。所以，可以采用此方式来进行优化。

因为 buffer pool 的使用，使得数据库的效率大大提高了，但是却失去了持久性，因为部分数据并没有切实写进磁盘中，而是放到了内存中。当服务突然宕机，会导致还没来得及刷盘的写入操作失效。

为了解决这个问题，又引入了 redo log。当事务提交时，会优先写入 redo log，然后再写入到 buffer pool 中。当服务突然宕机后，会根据 redolog 来进行数据恢复。

但是这又引入了新的问题，redolog 也是磁盘 io 操作，它的性能问题又怎么办呢？

* 首先，虽然 redolog 是 io 操作，但是它与 buffer pool 的不同在于，它是顺序 io，顺序 io 操作会明显快于随机 io。
* buffer pool 并不是以每条数据为单位的，而是为了效率以数据页为单位，MySQL 默认数据也大小为 16kb。一个 page 上有很多数据，任何一个小修改都会需要刷入磁盘。而 redolog 是以一条条操作为单位，不会写入其他数据，写入量大大减小。

当然了，尽管如此，每次操作都会附带 io 仍然会降低性能。为之，就像套娃一样，MySQL 又为 redolog 提供了不同的写入时机：

* 默认每次事务提交时写入磁盘
* 每次事务提交时写到系统页面缓存，不保证写入磁盘
* 每秒写入一次，也就是说 crash 仅最多丢失一秒的数据
